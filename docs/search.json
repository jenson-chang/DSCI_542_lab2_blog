[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Feature Intepretation for a k-Nearest Neighbour Classifier",
    "section": "",
    "text": "Understanding the inner workings of machine learning models is crucial in being able to explain the behaviour of the model. A k-Nearest Neighbour (kNN) model does not have coefficients like a linear model that can be used to explain feature importance. SHAP (SHapley Additive exPlanations) is a powerful tool for explaining model predictions, even for models that may seem like a “black box” such as kNN. This post will explain the steps needed to create a SHAP plots to interpret feature importance for using a kNN classifier in scikit-learn."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSCI 542 Lab 2 Blog",
    "section": "",
    "text": "Feature Interpretation for a k-Nearest Neighbor Classifier\n\n\n\n\n\n\nsklearn\n\n\n\n\n\n\n\n\n\nJan 14, 2025\n\n\nJenson Chang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html#computing-shap-values",
    "href": "posts/welcome/index.html#computing-shap-values",
    "title": "Feature Intepretation for a k-Nearest Neighbour Classifier",
    "section": "Computing SHAP values",
    "text": "Computing SHAP values\nThe scope of this post is to focus on SHAP plot generation and plot interpretation. Therefore we start with a pre-trained pipeline with a kNN model. The example model used is from UBC MDS Student’s Academic Success Prediction project. This model predicts whether a first year university student would dropout of school using academic and social indicators. Please refer to the GitHub page for more information on the dataset and how the model was developed.\n\nStep 1\nTo start, we first need perform some preparation steps. We need to first extract the prediction model knn from my_pipeline and transform our training data (X_train) and test data (X_test) with the preprocessor\n# Extract the KNN classifier from the pipeline\nknn = my_pipeline.named_steps['kneighborsclassifier']\n\n# Preprocess the training and testing data\nX_train_preprocessed = my_pipeline[:-1].transform(X_train)  # Apply all steps except the final classifier\nX_test_preprocessed = my_pipeline[:-1].transform(X_test)\n\n\nStep 2\nSince kNN does not provide direct interpretability, we’ll use SHAP’s KernelExplainer, which works with any model that has a predict_proba method.\n# Define SHAP Kernel Explainer with the predict_proba method\nexplainer = shap.KernelExplainer(knn.predict_proba, X_train_preprocessed)\n\n# Calculate SHAP values for the test dataset\nshap_values = explainer.shap_values(X_test_preprocessed)\nThe shap_values object now contains SHAP values for all classes. Each element in shap_values corresponds to one class.\n\n\nStep 3 (Optional)\nDepending on the size of the dataset, the KernelExplainer in the may give a warning about slow runtimes.\n\n\n\nWarning about long runtime from KernelExplainer\n\n\nTo reduce the runtime, we can subsample our data before passing it to KernelExplainer.\n# Reduce the dataset to 100 samples for KernelExplainer\nsubsample = shap.sample(X_train_preprocessed, 100)\nexplainer = shap.KernelExplainer(knn.predict_proba, subsample)\nshap_values = explainer.shap_values(X_test_preprocessed)\n\n\nStep 4\nTo focus on a specific class, select its SHAP values and generate a summary plot. In the example model, Class 0 corresponds Dropout which is the class we want to analyze.\nfeature_names = pipeline[:-1].get_feature_names_out()\nshap.summary_plot(shap_values[:,:,0], X_test_preprocessed, feature_names=feature_names)"
  },
  {
    "objectID": "posts/welcome/index.html#generating-shap-plot",
    "href": "posts/welcome/index.html#generating-shap-plot",
    "title": "Feature Intepretation for a k-Nearest Neighbour Classifier",
    "section": "Generating SHAP plot",
    "text": "Generating SHAP plot\n\nStep 1\nTo focus on Class 0, select its SHAP values and generate a summary plot:\nfeature_names = numeric_features\nshap.summary_plot(shap_values[:,:,0], X_test_preprocessed, feature_names=feature_names)\nThis plot will show: 1. Feature Importance: Features are sorted by their average impact on predictions for Class 0. 2. Feature Effects: Each dot represents a single sample, and its position along the X-axis shows the magnitude and direction of the feature’s effect. 3. Color Gradient: Indicates feature values (e.g., blue for low and red for high).\n\n\nStep 2\nUnderstanding the Components of the Plot 1. X-Axis (SHAP Value): Indicates how much a feature pushes the prediction for Class 0 (e.g., Setosa) higher (positive) or lower (negative). 2. Y-Axis (Features): Features are sorted by their importance for Class 0. 3. Color (Feature Value): Shows whether the feature value is high (red) or low (blue).\nExample Interpretation\n(table)\nKey Observations: • Petal Width (cm): The most important feature. Red dots (high petal widths) are mostly on the positive side, indicating that higher petal widths strongly favor Class 0. • Petal Length (cm): Similarly, high values (red) increase the likelihood of Class 0. • Sepal Length (cm): Acts oppositely; higher values reduce the probability of Class 0. • Sepal Width (cm): Its impact varies based on interactions with other features, as it has both positive and negative SHAP values.\nClass-Specific Insights\nBy focusing on Class 0, we learn that: • Features related to petal size are strong indicators of Class 0. • Sepal size, while less important, can either support or counter the prediction depending on its value."
  },
  {
    "objectID": "posts/welcome/index.html#conclusion",
    "href": "posts/welcome/index.html#conclusion",
    "title": "Feature Intepretation for a k-Nearest Neighbour Classifier",
    "section": "Conclusion",
    "text": "Conclusion\nSHAP plots provide a powerful way to demystify KNN classifiers, which are often considered black-box models. By analyzing SHAP values: • You can identify the most influential features for each class. • You can explain individual predictions in terms of feature contributions. • You can gain actionable insights into how the model distinguishes between classes.\nUsing the SHAP summary plot for Class 0, we identified that petal dimensions play a crucial role in classifying samples as Class 0 (Setosa), while sepals have a lesser but context-dependent effect.\nSHAP helps bridge the gap between machine learning models and human interpretability, making your models more transparent and trustworthy."
  },
  {
    "objectID": "posts/welcome/index.html#intepreting-shap-plot",
    "href": "posts/welcome/index.html#intepreting-shap-plot",
    "title": "Feature Intepretation for a k-Nearest Neighbour Classifier",
    "section": "Intepreting SHAP plot",
    "text": "Intepreting SHAP plot\n\n\nUnderstanding the Components of the Plot\n\nX-Axis (SHAP Value): Indicates how much a feature pushes the prediction towards the class Dropout.\nY-Axis (Features): Features are sorted by their importance for the class Dropout.\nColor (Feature Value): Shows whether the feature value is high (red) or low (blue).\n\n\n\nPlot Interpretation\n\nFeature Importance: Features are sorted by their average impact on predictions for the class Dropout.\nFeature Effects: Each dot represents a single sample, and its position along the X-axis shows the magnitude and direction of the feature’s effect.\nColor Gradient: Indicates feature values (e.g., blue for low and red for high).\n\nExample Interpretation\nCurricular units 2nd sem (grade):\nThis feature represents the average grade of the students in the 2nd semester. We see the students with higher grades (red) clustered on the left side, indicating students with higher grades reduces the prediction for dropout. The students with lower grades (blue) are clustered on the right, indicating they are more likely to push the model to predict dropout. We see there’s no overlap between the two groups in the middle. This show theres a distinct pattern and a strong relationship between 2nd semster grade and our predictor.\nAdmission Grade:\nThis feature represents the grade the student had at the time of admission. Similar to Curricular units 2nd sem (grade), we see a cluster of red points (higher grades) on the left and blue points (lower grades) on the right. However there’s an overlap of the two groups in the middle. The seperation between the two groups is not as strong as Curricular units 2nd sem (grade). Therefore the relative prediction power of Admission Grade is not as strong. The points in the middle are not pushing the prediction outcome to one class or another.\nCurricular units 1st sem (without evaluation)\nThis feature represents the number of non-evaluation courses (no exams) the student is taking. We no longer see a left/right clustering like in previous examples. The points in blue are students that are taking very little or none of these courses. These points are centered around 0 and not contributing much to our prediction. On the other hands, we see students who take a higher amount of these courses distributed evenly on the left and on the right. There’s no strong relationship between whether a student drops out and the number of non-evaluation courses that they take."
  },
  {
    "objectID": "posts/welcome/index.html#takeaway",
    "href": "posts/welcome/index.html#takeaway",
    "title": "Feature Intepretation for a k-Nearest Neighbour Classifier",
    "section": "Takeaway",
    "text": "Takeaway\nSHAP plots provide a powerful way to demystify KNN classifiers, which are often considered black-box models. By analyzing SHAP values, you can identify the most influential features for each class.\nSHAP helps bridge the gap between machine learning models and human interpretability, making your models more transparent and trustworthy."
  },
  {
    "objectID": "posts/shap/index.html",
    "href": "posts/shap/index.html",
    "title": "Feature Interpretation for a k-Nearest Neighbor Classifier",
    "section": "",
    "text": "Understanding the inner workings of machine learning models is crucial in being able to explain the behavior of the model. A k-Nearest Neighbor (kNN) model does not have coefficients like a linear model that can be used to explain feature importance. SHAP (SHapley Additive explanations) is a powerful tool for explaining model predictions, even for models that may seem like a “black box” such as kNN. This post will explain the steps needed to create a SHAP plots to interpret feature importance for using a kNN classifier in scikit-learn."
  },
  {
    "objectID": "posts/shap/index.html#computing-shap-values",
    "href": "posts/shap/index.html#computing-shap-values",
    "title": "Feature Interpretation for a k-Nearest Neighbor Classifier",
    "section": "Computing SHAP values",
    "text": "Computing SHAP values\nThe scope of this post is to focus on SHAP plot generation and plot interpretation. Therefore we start with a pre-trained pipeline with a kNN model. The example model used is from UBC MDS Student’s Academic Success Prediction project. This model predicts whether a first year university student would dropout of school using academic and social indicators. Please refer to the GitHub page for more information on the dataset and how the model was developed.\n\nStep 1. Data Preparation\nTo start, we first need perform some preparation steps. We need to first extract the prediction model knn from my_pipeline and transform our training data (X_train) and test data (X_test) with the preprocessor\n# Extract the KNN classifier from the pipeline\nknn = my_pipeline.named_steps['kneighborsclassifier']\n\n# Preprocess the training and testing data\nX_train_preprocessed = my_pipeline[:-1].transform(X_train)  # Apply all steps except the final classifier\nX_test_preprocessed = my_pipeline[:-1].transform(X_test)\n\n\nStep 2. SHAP Values\nSince kNN does not provide direct interpretability, we’ll use SHAP’s KernelExplainer, which works with any model that has a predict_proba method.\n# Define SHAP Kernel Explainer with the predict_proba method\nexplainer = shap.KernelExplainer(knn.predict_proba, X_train_preprocessed)\n\n# Calculate SHAP values for the test dataset\nshap_values = explainer.shap_values(X_test_preprocessed)\nThe shap_values object now contains SHAP values for all classes. Each element in shap_values corresponds to one class.\n\n\nStep 3. Subsample (Optional)\nDepending on the size of the dataset, the KernelExplainer in the may give a warning about slow runtimes.\n\n\n\nWarning about long runtime from KernelExplainer\n\n\nTo reduce the runtime, we can subsample our data before passing it to KernelExplainer.\n# Reduce the dataset to 100 samples for KernelExplainer\nsubsample = shap.sample(X_train_preprocessed, 100)\nexplainer = shap.KernelExplainer(knn.predict_proba, subsample)\nshap_values = explainer.shap_values(X_test_preprocessed)\n\n\nStep 4\nTo focus on a specific class, select its SHAP values and generate a summary plot. In the example model, Class 0 corresponds Dropout which is the class we want to analyze.\nfeature_names = pipeline[:-1].get_feature_names_out()\nshap.summary_plot(shap_values[:,:,0], X_test_preprocessed, feature_names=feature_names)"
  },
  {
    "objectID": "posts/shap/index.html#intepreting-shap-plot",
    "href": "posts/shap/index.html#intepreting-shap-plot",
    "title": "Feature Interpretation for a k-Nearest Neighbor Classifier",
    "section": "Intepreting SHAP plot",
    "text": "Intepreting SHAP plot\n\n\nUnderstanding the Components of the Plot\n\nX-Axis (SHAP Value): Indicates how much a feature pushes the prediction towards the class Dropout.\nY-Axis (Features): Features are sorted by their importance for the class Dropout.\nColor (Feature Value): Shows whether the feature value is high (red) or low (blue).\n\n\n\nPlot Interpretation\n\nFeature Importance: Features are sorted by their average impact on predictions for the class Dropout.\nFeature Effects: Each dot represents a single sample, and its position along the X-axis shows the magnitude and direction of the feature’s effect.\nColor Gradient: Indicates feature values (e.g., blue for low and red for high).\n\nExample 1\n\nThis feature represents the average grade of the students in the 2nd semester. We see the students with higher grades (red) clustered on the left side, indicating students with higher grades reduces the prediction for dropout. The students with lower grades (blue) are clustered on the right, indicating they are more likely to push the model to predict dropout. We see there’s no overlap between the two groups in the middle. This show theres a distinct pattern and a strong relationship between 2nd semster grade and our predictor.\nExample 2\n\nThis feature represents the grade the student had at the time of admission. Similar to Curricular units 2nd sem (grade), we see a cluster of red points (higher grades) on the left and blue points (lower grades) on the right. However there’s an overlap of the two groups in the middle. The seperation between the two groups is not as strong as Curricular units 2nd sem (grade). Therefore the relative prediction power of Admission Grade is not as strong. The points in the middle are not pushing the prediction outcome to one class or another.\nExample 3\n\nThis feature represents the number of non-evaluation courses (no exams) the student is taking. We no longer see a left/right clustering like in previous examples. The points in blue are students that are taking very little or none of these courses. These points are centered around 0 and not contributing much to our prediction. On the other hands, we see students who take a higher amount of these courses distributed evenly on the left and on the right. There’s no strong relationship between whether a student drops out and the number of non-evaluation courses that they take."
  },
  {
    "objectID": "posts/shap/index.html#takeaway",
    "href": "posts/shap/index.html#takeaway",
    "title": "Feature Interpretation for a k-Nearest Neighbor Classifier",
    "section": "Takeaway",
    "text": "Takeaway\nSHAP plots provide a powerful way to demystify KNN classifiers, which are often considered black-box models. By analyzing SHAP values, we can identify the most influential features for each class.\nSHAP helps bridge the gap between machine learning models and human interpretability, making models more transparent and trustworthy."
  },
  {
    "objectID": "posts/shap/index.html#references",
    "href": "posts/shap/index.html#references",
    "title": "Feature Interpretation for a k-Nearest Neighbor Classifier",
    "section": "References",
    "text": "References\n“Academic Success Prediction.” https://github.com/UBC-MDS/academic-success-prediction\nSHAP. “Iris Classification with Scikit-Learn: K-Nearest Neighbors.” SHAP Documentation. https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Iris%20classification%20with%20scikit-learn.html#K-nearest-neighbors."
  }
]